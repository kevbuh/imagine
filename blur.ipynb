{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'maybe_allow_in_graph' from 'diffusers.utils' (/Users/kevinbuhler/opt/anaconda3/envs/blur/lib/python3.11/site-packages/diffusers/utils/__init__.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 19\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mPIL\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mImage\u001b[39;00m\n\u001b[1;32m     17\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mnumpy\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mnp\u001b[39;00m\n\u001b[0;32m---> 19\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mdiffusers\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mpipelines\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mstable_diffusion\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mpipeline_stable_diffusion_controlnet\u001b[39;00m \u001b[39mimport\u001b[39;00m \u001b[39m*\u001b[39m\n\u001b[1;32m     21\u001b[0m EXAMPLE_DOC_STRING \u001b[39m=\u001b[39m \u001b[39m\"\"\"\u001b[39m\n\u001b[1;32m     22\u001b[0m \u001b[39m    Examples:\u001b[39m\n\u001b[1;32m     23\u001b[0m \u001b[39m        ```py\u001b[39m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     70\u001b[0m \u001b[39m        ```\u001b[39m\n\u001b[1;32m     71\u001b[0m \u001b[39m\"\"\"\u001b[39m\n\u001b[1;32m     74\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mprepare_mask_and_masked_image\u001b[39m(image, mask):\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/blur/lib/python3.11/site-packages/diffusers/pipelines/stable_diffusion/pipeline_stable_diffusion_controlnet.py:18\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39m# Copyright 2023 The HuggingFace Team. All rights reserved.\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[39m#\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[39m# Licensed under the Apache License, Version 2.0 (the \"License\");\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[39m# NOTE: This file is deprecated and will be removed in a future version.\u001b[39;00m\n\u001b[1;32m     16\u001b[0m \u001b[39m# It only exists so that temporarely `from diffusers.pipelines import DiffusionPipeline` works\u001b[39;00m\n\u001b[1;32m     17\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mutils\u001b[39;00m \u001b[39mimport\u001b[39;00m deprecate\n\u001b[0;32m---> 18\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mcontrolnet\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mmulticontrolnet\u001b[39;00m \u001b[39mimport\u001b[39;00m MultiControlNetModel  \u001b[39m# noqa: F401\u001b[39;00m\n\u001b[1;32m     19\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mcontrolnet\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mpipeline_controlnet\u001b[39;00m \u001b[39mimport\u001b[39;00m StableDiffusionControlNetPipeline  \u001b[39m# noqa: F401\u001b[39;00m\n\u001b[1;32m     22\u001b[0m deprecate(\n\u001b[1;32m     23\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mstable diffusion controlnet\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[1;32m     24\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39m0.22.0\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     27\u001b[0m     stacklevel\u001b[39m=\u001b[39m\u001b[39m3\u001b[39m,\n\u001b[1;32m     28\u001b[0m )\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/blur/lib/python3.11/site-packages/diffusers/pipelines/controlnet/multicontrolnet.py:6\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mtorch\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtorch\u001b[39;00m \u001b[39mimport\u001b[39;00m nn\n\u001b[0;32m----> 6\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mmodels\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mcontrolnet\u001b[39;00m \u001b[39mimport\u001b[39;00m ControlNetModel, ControlNetOutput\n\u001b[1;32m      7\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mmodels\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mmodeling_utils\u001b[39;00m \u001b[39mimport\u001b[39;00m ModelMixin\n\u001b[1;32m     10\u001b[0m \u001b[39mclass\u001b[39;00m \u001b[39mMultiControlNetModel\u001b[39;00m(ModelMixin):\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/blur/lib/python3.11/site-packages/diffusers/models/controlnet.py:23\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mconfiguration_utils\u001b[39;00m \u001b[39mimport\u001b[39;00m ConfigMixin, register_to_config\n\u001b[1;32m     22\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mutils\u001b[39;00m \u001b[39mimport\u001b[39;00m BaseOutput, logging\n\u001b[0;32m---> 23\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39mattention_processor\u001b[39;00m \u001b[39mimport\u001b[39;00m AttentionProcessor, AttnProcessor\n\u001b[1;32m     24\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39membeddings\u001b[39;00m \u001b[39mimport\u001b[39;00m TimestepEmbedding, Timesteps\n\u001b[1;32m     25\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39mmodeling_utils\u001b[39;00m \u001b[39mimport\u001b[39;00m ModelMixin\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/blur/lib/python3.11/site-packages/diffusers/models/attention_processor.py:20\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mtorch\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mnn\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mfunctional\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mF\u001b[39;00m\n\u001b[1;32m     18\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtorch\u001b[39;00m \u001b[39mimport\u001b[39;00m nn\n\u001b[0;32m---> 20\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mutils\u001b[39;00m \u001b[39mimport\u001b[39;00m deprecate, logging, maybe_allow_in_graph\n\u001b[1;32m     21\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mutils\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mimport_utils\u001b[39;00m \u001b[39mimport\u001b[39;00m is_xformers_available\n\u001b[1;32m     24\u001b[0m logger \u001b[39m=\u001b[39m logging\u001b[39m.\u001b[39mget_logger(\u001b[39m__name__\u001b[39m)  \u001b[39m# pylint: disable=invalid-name\u001b[39;00m\n",
      "\u001b[0;31mImportError\u001b[0m: cannot import name 'maybe_allow_in_graph' from 'diffusers.utils' (/Users/kevinbuhler/opt/anaconda3/envs/blur/lib/python3.11/site-packages/diffusers/utils/__init__.py)"
     ]
    }
   ],
   "source": [
    "# Copyright 2023 The HuggingFace Team. All rights reserved.\n",
    "#\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "#     http://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License.\n",
    "\n",
    "import torch\n",
    "import PIL.Image\n",
    "import numpy as np\n",
    "\n",
    "from diffusers.pipelines.stable_diffusion.pipeline_stable_diffusion_controlnet import *\n",
    "\n",
    "EXAMPLE_DOC_STRING = \"\"\"\n",
    "    Examples:\n",
    "        ```py\n",
    "        >>> # !pip install opencv-python transformers accelerate\n",
    "        >>> from diffusers import StableDiffusionControlNetInpaintPipeline, ControlNetModel, UniPCMultistepScheduler\n",
    "        >>> from diffusers.utils import load_image\n",
    "        >>> import numpy as np\n",
    "        >>> import torch\n",
    "\n",
    "        >>> import cv2\n",
    "        >>> from PIL import Image\n",
    "        >>> # download an image\n",
    "        >>> image = load_image(\n",
    "        ...     \"https://raw.githubusercontent.com/CompVis/latent-diffusion/main/data/inpainting_examples/overture-creations-5sI6fQgYIuo.png\"\n",
    "        ... )\n",
    "        >>> image = np.array(image)\n",
    "        >>> mask_image = load_image(\n",
    "        ...     \"https://raw.githubusercontent.com/CompVis/latent-diffusion/main/data/inpainting_examples/overture-creations-5sI6fQgYIuo_mask.png\"\n",
    "        ... )\n",
    "        >>> mask_image = np.array(mask_image)\n",
    "        >>> # get canny image\n",
    "        >>> canny_image = cv2.Canny(image, 100, 200)\n",
    "        >>> canny_image = canny_image[:, :, None]\n",
    "        >>> canny_image = np.concatenate([canny_image, canny_image, canny_image], axis=2)\n",
    "        >>> canny_image = Image.fromarray(canny_image)\n",
    "\n",
    "        >>> # load control net and stable diffusion v1-5\n",
    "        >>> controlnet = ControlNetModel.from_pretrained(\"lllyasviel/sd-controlnet-canny\", torch_dtype=torch.float16)\n",
    "        >>> pipe = StableDiffusionControlNetInpaintPipeline.from_pretrained(\n",
    "        ...     \"runwayml/stable-diffusion-inpainting\", controlnet=controlnet, torch_dtype=torch.float16\n",
    "        ... )\n",
    "\n",
    "        >>> # speed up diffusion process with faster scheduler and memory optimization\n",
    "        >>> pipe.scheduler = UniPCMultistepScheduler.from_config(pipe.scheduler.config)\n",
    "        >>> # remove following line if xformers is not installed\n",
    "        >>> pipe.enable_xformers_memory_efficient_attention()\n",
    "\n",
    "        >>> pipe.enable_model_cpu_offload()\n",
    "\n",
    "        >>> # generate image\n",
    "        >>> generator = torch.manual_seed(0)\n",
    "        >>> image = pipe(\n",
    "        ...     \"futuristic-looking doggo\",\n",
    "        ...     num_inference_steps=20,\n",
    "        ...     generator=generator,\n",
    "        ...     image=image,\n",
    "        ...     control_image=canny_image,\n",
    "        ...     mask_image=mask_image\n",
    "        ... ).images[0]\n",
    "        ```\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "def prepare_mask_and_masked_image(image, mask):\n",
    "    \"\"\"\n",
    "    Prepares a pair (image, mask) to be consumed by the Stable Diffusion pipeline. This means that those inputs will be\n",
    "    converted to ``torch.Tensor`` with shapes ``batch x channels x height x width`` where ``channels`` is ``3`` for the\n",
    "    ``image`` and ``1`` for the ``mask``.\n",
    "    The ``image`` will be converted to ``torch.float32`` and normalized to be in ``[-1, 1]``. The ``mask`` will be\n",
    "    binarized (``mask > 0.5``) and cast to ``torch.float32`` too.\n",
    "    Args:\n",
    "        image (Union[np.array, PIL.Image, torch.Tensor]): The image to inpaint.\n",
    "            It can be a ``PIL.Image``, or a ``height x width x 3`` ``np.array`` or a ``channels x height x width``\n",
    "            ``torch.Tensor`` or a ``batch x channels x height x width`` ``torch.Tensor``.\n",
    "        mask (_type_): The mask to apply to the image, i.e. regions to inpaint.\n",
    "            It can be a ``PIL.Image``, or a ``height x width`` ``np.array`` or a ``1 x height x width``\n",
    "            ``torch.Tensor`` or a ``batch x 1 x height x width`` ``torch.Tensor``.\n",
    "    Raises:\n",
    "        ValueError: ``torch.Tensor`` images should be in the ``[-1, 1]`` range. ValueError: ``torch.Tensor`` mask\n",
    "        should be in the ``[0, 1]`` range. ValueError: ``mask`` and ``image`` should have the same spatial dimensions.\n",
    "        TypeError: ``mask`` is a ``torch.Tensor`` but ``image`` is not\n",
    "            (ot the other way around).\n",
    "    Returns:\n",
    "        tuple[torch.Tensor]: The pair (mask, masked_image) as ``torch.Tensor`` with 4\n",
    "            dimensions: ``batch x channels x height x width``.\n",
    "    \"\"\"\n",
    "    if isinstance(image, torch.Tensor):\n",
    "        if not isinstance(mask, torch.Tensor):\n",
    "            raise TypeError(f\"`image` is a torch.Tensor but `mask` (type: {type(mask)} is not\")\n",
    "\n",
    "        # Batch single image\n",
    "        if image.ndim == 3:\n",
    "            assert image.shape[0] == 3, \"Image outside a batch should be of shape (3, H, W)\"\n",
    "            image = image.unsqueeze(0)\n",
    "\n",
    "        # Batch and add channel dim for single mask\n",
    "        if mask.ndim == 2:\n",
    "            mask = mask.unsqueeze(0).unsqueeze(0)\n",
    "\n",
    "        # Batch single mask or add channel dim\n",
    "        if mask.ndim == 3:\n",
    "            # Single batched mask, no channel dim or single mask not batched but channel dim\n",
    "            if mask.shape[0] == 1:\n",
    "                mask = mask.unsqueeze(0)\n",
    "\n",
    "            # Batched masks no channel dim\n",
    "            else:\n",
    "                mask = mask.unsqueeze(1)\n",
    "\n",
    "        assert image.ndim == 4 and mask.ndim == 4, \"Image and Mask must have 4 dimensions\"\n",
    "        assert image.shape[-2:] == mask.shape[-2:], \"Image and Mask must have the same spatial dimensions\"\n",
    "        assert image.shape[0] == mask.shape[0], \"Image and Mask must have the same batch size\"\n",
    "\n",
    "        # Check image is in [-1, 1]\n",
    "        if image.min() < -1 or image.max() > 1:\n",
    "            raise ValueError(\"Image should be in [-1, 1] range\")\n",
    "\n",
    "        # Check mask is in [0, 1]\n",
    "        if mask.min() < 0 or mask.max() > 1:\n",
    "            raise ValueError(\"Mask should be in [0, 1] range\")\n",
    "\n",
    "        # Binarize mask\n",
    "        mask[mask < 0.5] = 0\n",
    "        mask[mask >= 0.5] = 1\n",
    "\n",
    "        # Image as float32\n",
    "        image = image.to(dtype=torch.float32)\n",
    "    elif isinstance(mask, torch.Tensor):\n",
    "        raise TypeError(f\"`mask` is a torch.Tensor but `image` (type: {type(image)} is not\")\n",
    "    else:\n",
    "        # preprocess image\n",
    "        if isinstance(image, (PIL.Image.Image, np.ndarray)):\n",
    "            image = [image]\n",
    "\n",
    "        if isinstance(image, list) and isinstance(image[0], PIL.Image.Image):\n",
    "            image = [np.array(i.convert(\"RGB\"))[None, :] for i in image]\n",
    "            image = np.concatenate(image, axis=0)\n",
    "        elif isinstance(image, list) and isinstance(image[0], np.ndarray):\n",
    "            image = np.concatenate([i[None, :] for i in image], axis=0)\n",
    "\n",
    "        image = image.transpose(0, 3, 1, 2)\n",
    "        image = torch.from_numpy(image).to(dtype=torch.float32) / 127.5 - 1.0\n",
    "\n",
    "        # preprocess mask\n",
    "        if isinstance(mask, (PIL.Image.Image, np.ndarray)):\n",
    "            mask = [mask]\n",
    "\n",
    "        if isinstance(mask, list) and isinstance(mask[0], PIL.Image.Image):\n",
    "            mask = np.concatenate([np.array(m.convert(\"L\"))[None, None, :] for m in mask], axis=0)\n",
    "            mask = mask.astype(np.float32) / 255.0\n",
    "        elif isinstance(mask, list) and isinstance(mask[0], np.ndarray):\n",
    "            mask = np.concatenate([m[None, None, :] for m in mask], axis=0)\n",
    "\n",
    "        mask[mask < 0.5] = 0\n",
    "        mask[mask >= 0.5] = 1\n",
    "        mask = torch.from_numpy(mask)\n",
    "\n",
    "    masked_image = image * (mask < 0.5)\n",
    "\n",
    "    return mask, masked_image\n",
    "\n",
    "class StableDiffusionControlNetInpaintPipeline(StableDiffusionControlNetPipeline):\n",
    "    r\"\"\"\n",
    "    Pipeline for text-guided image inpainting using Stable Diffusion with ControlNet guidance.\n",
    "\n",
    "    This model inherits from [`StableDiffusionControlNetPipeline`]. Check the superclass documentation for the generic methods the\n",
    "    library implements for all the pipelines (such as downloading or saving, running on a particular device, etc.)\n",
    "\n",
    "    Args:\n",
    "        vae ([`AutoencoderKL`]):\n",
    "            Variational Auto-Encoder (VAE) Model to encode and decode images to and from latent representations.\n",
    "        text_encoder ([`CLIPTextModel`]):\n",
    "            Frozen text-encoder. Stable Diffusion uses the text portion of\n",
    "            [CLIP](https://huggingface.co/docs/transformers/model_doc/clip#transformers.CLIPTextModel), specifically\n",
    "            the [clip-vit-large-patch14](https://huggingface.co/openai/clip-vit-large-patch14) variant.\n",
    "        tokenizer (`CLIPTokenizer`):\n",
    "            Tokenizer of class\n",
    "            [CLIPTokenizer](https://huggingface.co/docs/transformers/v4.21.0/en/model_doc/clip#transformers.CLIPTokenizer).\n",
    "        unet ([`UNet2DConditionModel`]): Conditional U-Net architecture to denoise the encoded image latents.\n",
    "        controlnet ([`ControlNetModel`]):\n",
    "            Provides additional conditioning to the unet during the denoising process\n",
    "        scheduler ([`SchedulerMixin`]):\n",
    "            A scheduler to be used in combination with `unet` to denoise the encoded image latents. Can be one of\n",
    "            [`DDIMScheduler`], [`LMSDiscreteScheduler`], or [`PNDMScheduler`].\n",
    "        safety_checker ([`StableDiffusionSafetyChecker`]):\n",
    "            Classification module that estimates whether generated images could be considered offensive or harmful.\n",
    "            Please, refer to the [model card](https://huggingface.co/runwayml/stable-diffusion-v1-5) for details.\n",
    "        feature_extractor ([`CLIPFeatureExtractor`]):\n",
    "            Model that extracts features from generated images to be used as inputs for the `safety_checker`.\n",
    "    \"\"\"\n",
    "    \n",
    "    def prepare_mask_latents(\n",
    "        self, mask, masked_image, batch_size, height, width, dtype, device, generator, do_classifier_free_guidance\n",
    "    ):\n",
    "        # resize the mask to latents shape as we concatenate the mask to the latents\n",
    "        # we do that before converting to dtype to avoid breaking in case we're using cpu_offload\n",
    "        # and half precision\n",
    "        mask = torch.nn.functional.interpolate(\n",
    "            mask, size=(height // self.vae_scale_factor, width // self.vae_scale_factor)\n",
    "        )\n",
    "        mask = mask.to(device=device, dtype=dtype)\n",
    "\n",
    "        masked_image = masked_image.to(device=device, dtype=dtype)\n",
    "\n",
    "        # encode the mask image into latents space so we can concatenate it to the latents\n",
    "        if isinstance(generator, list):\n",
    "            masked_image_latents = [\n",
    "                self.vae.encode(masked_image[i : i + 1]).latent_dist.sample(generator=generator[i])\n",
    "                for i in range(batch_size)\n",
    "            ]\n",
    "            masked_image_latents = torch.cat(masked_image_latents, dim=0)\n",
    "        else:\n",
    "            masked_image_latents = self.vae.encode(masked_image).latent_dist.sample(generator=generator)\n",
    "        masked_image_latents = self.vae.config.scaling_factor * masked_image_latents\n",
    "\n",
    "        # duplicate mask and masked_image_latents for each generation per prompt, using mps friendly method\n",
    "        if mask.shape[0] < batch_size:\n",
    "            if not batch_size % mask.shape[0] == 0:\n",
    "                raise ValueError(\n",
    "                    \"The passed mask and the required batch size don't match. Masks are supposed to be duplicated to\"\n",
    "                    f\" a total batch size of {batch_size}, but {mask.shape[0]} masks were passed. Make sure the number\"\n",
    "                    \" of masks that you pass is divisible by the total requested batch size.\"\n",
    "                )\n",
    "            mask = mask.repeat(batch_size // mask.shape[0], 1, 1, 1)\n",
    "        if masked_image_latents.shape[0] < batch_size:\n",
    "            if not batch_size % masked_image_latents.shape[0] == 0:\n",
    "                raise ValueError(\n",
    "                    \"The passed images and the required batch size don't match. Images are supposed to be duplicated\"\n",
    "                    f\" to a total batch size of {batch_size}, but {masked_image_latents.shape[0]} images were passed.\"\n",
    "                    \" Make sure the number of images that you pass is divisible by the total requested batch size.\"\n",
    "                )\n",
    "            masked_image_latents = masked_image_latents.repeat(batch_size // masked_image_latents.shape[0], 1, 1, 1)\n",
    "\n",
    "        mask = torch.cat([mask] * 2) if do_classifier_free_guidance else mask\n",
    "        masked_image_latents = (\n",
    "            torch.cat([masked_image_latents] * 2) if do_classifier_free_guidance else masked_image_latents\n",
    "        )\n",
    "\n",
    "        # aligning device to prevent device errors when concating it with the latent model input\n",
    "        masked_image_latents = masked_image_latents.to(device=device, dtype=dtype)\n",
    "        return mask, masked_image_latents\n",
    "    \n",
    "    @torch.no_grad()\n",
    "    @replace_example_docstring(EXAMPLE_DOC_STRING)\n",
    "    def __call__(\n",
    "        self,\n",
    "        prompt: Union[str, List[str]] = None,        \n",
    "        image: Union[torch.FloatTensor, PIL.Image.Image] = None,\n",
    "        control_image: Union[torch.FloatTensor, PIL.Image.Image, List[torch.FloatTensor], List[PIL.Image.Image]] = None,        \n",
    "        mask_image: Union[torch.FloatTensor, PIL.Image.Image] = None,\n",
    "        height: Optional[int] = None,\n",
    "        width: Optional[int] = None,\n",
    "        num_inference_steps: int = 50,\n",
    "        guidance_scale: float = 7.5,\n",
    "        negative_prompt: Optional[Union[str, List[str]]] = None,\n",
    "        num_images_per_prompt: Optional[int] = 1,\n",
    "        eta: float = 0.0,\n",
    "        generator: Optional[Union[torch.Generator, List[torch.Generator]]] = None,\n",
    "        latents: Optional[torch.FloatTensor] = None,\n",
    "        prompt_embeds: Optional[torch.FloatTensor] = None,\n",
    "        negative_prompt_embeds: Optional[torch.FloatTensor] = None,\n",
    "        output_type: Optional[str] = \"pil\",\n",
    "        return_dict: bool = True,\n",
    "        callback: Optional[Callable[[int, int, torch.FloatTensor], None]] = None,\n",
    "        callback_steps: int = 1,\n",
    "        cross_attention_kwargs: Optional[Dict[str, Any]] = None,\n",
    "        controlnet_conditioning_scale: float = 1.0,\n",
    "    ):\n",
    "        r\"\"\"\n",
    "        Function invoked when calling the pipeline for generation.\n",
    "        Args:\n",
    "            prompt (`str` or `List[str]`, *optional*):\n",
    "                The prompt or prompts to guide the image generation. If not defined, one has to pass `prompt_embeds`.\n",
    "                instead.\n",
    "            image (`PIL.Image.Image`):\n",
    "                `Image`, or tensor representing an image batch which will be inpainted, *i.e.* parts of the image will\n",
    "                be masked out with `mask_image` and repainted according to `prompt`.\n",
    "            control_image (`torch.FloatTensor`, `PIL.Image.Image`, `List[torch.FloatTensor]` or `List[PIL.Image.Image]`):\n",
    "                The ControlNet input condition. ControlNet uses this input condition to generate guidance to Unet. If\n",
    "                the type is specified as `Torch.FloatTensor`, it is passed to ControlNet as is. PIL.Image.Image` can\n",
    "                also be accepted as an image. The control image is automatically resized to fit the output image.\n",
    "            mask_image (`PIL.Image.Image`):\n",
    "                `Image`, or tensor representing an image batch, to mask `image`. White pixels in the mask will be\n",
    "                repainted, while black pixels will be preserved. If `mask_image` is a PIL image, it will be converted\n",
    "                to a single channel (luminance) before use. If it's a tensor, it should contain one color channel (L)\n",
    "                instead of 3, so the expected shape would be `(B, H, W, 1)`.            \n",
    "            height (`int`, *optional*, defaults to self.unet.config.sample_size * self.vae_scale_factor):\n",
    "                The height in pixels of the generated image.\n",
    "            width (`int`, *optional*, defaults to self.unet.config.sample_size * self.vae_scale_factor):\n",
    "                The width in pixels of the generated image.\n",
    "            num_inference_steps (`int`, *optional*, defaults to 50):\n",
    "                The number of denoising steps. More denoising steps usually lead to a higher quality image at the\n",
    "                expense of slower inference.\n",
    "            guidance_scale (`float`, *optional*, defaults to 7.5):\n",
    "                Guidance scale as defined in [Classifier-Free Diffusion Guidance](https://arxiv.org/abs/2207.12598).\n",
    "                `guidance_scale` is defined as `w` of equation 2. of [Imagen\n",
    "                Paper](https://arxiv.org/pdf/2205.11487.pdf). Guidance scale is enabled by setting `guidance_scale >\n",
    "                1`. Higher guidance scale encourages to generate images that are closely linked to the text `prompt`,\n",
    "                usually at the expense of lower image quality.\n",
    "            negative_prompt (`str` or `List[str]`, *optional*):\n",
    "                The prompt or prompts not to guide the image generation. If not defined, one has to pass\n",
    "                `negative_prompt_embeds`. instead. If not defined, one has to pass `negative_prompt_embeds`. instead.\n",
    "                Ignored when not using guidance (i.e., ignored if `guidance_scale` is less than `1`).\n",
    "            num_images_per_prompt (`int`, *optional*, defaults to 1):\n",
    "                The number of images to generate per prompt.\n",
    "            eta (`float`, *optional*, defaults to 0.0):\n",
    "                Corresponds to parameter eta (η) in the DDIM paper: https://arxiv.org/abs/2010.02502. Only applies to\n",
    "                [`schedulers.DDIMScheduler`], will be ignored for others.\n",
    "            generator (`torch.Generator` or `List[torch.Generator]`, *optional*):\n",
    "                One or a list of [torch generator(s)](https://pytorch.org/docs/stable/generated/torch.Generator.html)\n",
    "                to make generation deterministic.\n",
    "            latents (`torch.FloatTensor`, *optional*):\n",
    "                Pre-generated noisy latents, sampled from a Gaussian distribution, to be used as inputs for image\n",
    "                generation. Can be used to tweak the same generation with different prompts. If not provided, a latents\n",
    "                tensor will ge generated by sampling using the supplied random `generator`.\n",
    "            prompt_embeds (`torch.FloatTensor`, *optional*):\n",
    "                Pre-generated text embeddings. Can be used to easily tweak text inputs, *e.g.* prompt weighting. If not\n",
    "                provided, text embeddings will be generated from `prompt` input argument.\n",
    "            negative_prompt_embeds (`torch.FloatTensor`, *optional*):\n",
    "                Pre-generated negative text embeddings. Can be used to easily tweak text inputs, *e.g.* prompt\n",
    "                weighting. If not provided, negative_prompt_embeds will be generated from `negative_prompt` input\n",
    "                argument.\n",
    "            output_type (`str`, *optional*, defaults to `\"pil\"`):\n",
    "                The output format of the generate image. Choose between\n",
    "                [PIL](https://pillow.readthedocs.io/en/stable/): `PIL.Image.Image` or `np.array`.\n",
    "            return_dict (`bool`, *optional*, defaults to `True`):\n",
    "                Whether or not to return a [`~pipelines.stable_diffusion.StableDiffusionPipelineOutput`] instead of a\n",
    "                plain tuple.\n",
    "            callback (`Callable`, *optional*):\n",
    "                A function that will be called every `callback_steps` steps during inference. The function will be\n",
    "                called with the following arguments: `callback(step: int, timestep: int, latents: torch.FloatTensor)`.\n",
    "            callback_steps (`int`, *optional*, defaults to 1):\n",
    "                The frequency at which the `callback` function will be called. If not specified, the callback will be\n",
    "                called at every step.\n",
    "            cross_attention_kwargs (`dict`, *optional*):\n",
    "                A kwargs dictionary that if specified is passed along to the `AttnProcessor` as defined under\n",
    "                `self.processor` in\n",
    "                [diffusers.cross_attention](https://github.com/huggingface/diffusers/blob/main/src/diffusers/models/cross_attention.py).\n",
    "            controlnet_conditioning_scale (`float`, *optional*, defaults to 1.0):\n",
    "                The outputs of the controlnet are multiplied by `controlnet_conditioning_scale` before they are added\n",
    "                to the residual in the original unet.\n",
    "        Examples:\n",
    "        Returns:\n",
    "            [`~pipelines.stable_diffusion.StableDiffusionPipelineOutput`] or `tuple`:\n",
    "            [`~pipelines.stable_diffusion.StableDiffusionPipelineOutput`] if `return_dict` is True, otherwise a `tuple.\n",
    "            When returning a tuple, the first element is a list with the generated images, and the second element is a\n",
    "            list of `bool`s denoting whether the corresponding generated image likely represents \"not-safe-for-work\"\n",
    "            (nsfw) content, according to the `safety_checker`.\n",
    "        \"\"\"\n",
    "        # 0. Default height and width to unet\n",
    "        height, width = self._default_height_width(height, width, control_image)\n",
    "\n",
    "        # 1. Check inputs. Raise error if not correct\n",
    "        self.check_inputs(\n",
    "            prompt, control_image, height, width, callback_steps, negative_prompt, prompt_embeds, negative_prompt_embeds\n",
    "        )\n",
    "\n",
    "        # 2. Define call parameters\n",
    "        if prompt is not None and isinstance(prompt, str):\n",
    "            batch_size = 1\n",
    "        elif prompt is not None and isinstance(prompt, list):\n",
    "            batch_size = len(prompt)\n",
    "        else:\n",
    "            batch_size = prompt_embeds.shape[0]\n",
    "\n",
    "        device = self._execution_device\n",
    "        # here `guidance_scale` is defined analog to the guidance weight `w` of equation (2)\n",
    "        # of the Imagen paper: https://arxiv.org/pdf/2205.11487.pdf . `guidance_scale = 1`\n",
    "        # corresponds to doing no classifier free guidance.\n",
    "        do_classifier_free_guidance = guidance_scale > 1.0\n",
    "\n",
    "        # 3. Encode input prompt\n",
    "        prompt_embeds = self._encode_prompt(\n",
    "            prompt,\n",
    "            device,\n",
    "            num_images_per_prompt,\n",
    "            do_classifier_free_guidance,\n",
    "            negative_prompt,\n",
    "            prompt_embeds=prompt_embeds,\n",
    "            negative_prompt_embeds=negative_prompt_embeds,\n",
    "        )\n",
    "\n",
    "        # 4. Prepare image\n",
    "        control_image = self.prepare_image(\n",
    "            control_image,\n",
    "            width,\n",
    "            height,\n",
    "            batch_size * num_images_per_prompt,\n",
    "            num_images_per_prompt,\n",
    "            device,\n",
    "            self.controlnet.dtype,\n",
    "        )\n",
    "\n",
    "        if do_classifier_free_guidance:\n",
    "            control_image = torch.cat([control_image] * 2)\n",
    "\n",
    "        # 5. Prepare timesteps\n",
    "        self.scheduler.set_timesteps(num_inference_steps, device=device)\n",
    "        timesteps = self.scheduler.timesteps\n",
    "\n",
    "        # 6. Prepare latent variables\n",
    "        num_channels_latents = self.controlnet.config.in_channels\n",
    "        latents = self.prepare_latents(\n",
    "            batch_size * num_images_per_prompt,\n",
    "            num_channels_latents,\n",
    "            height,\n",
    "            width,\n",
    "            prompt_embeds.dtype,\n",
    "            device,\n",
    "            generator,\n",
    "            latents,\n",
    "        )\n",
    "        \n",
    "        # EXTRA: prepare mask latents\n",
    "        mask, masked_image = prepare_mask_and_masked_image(image, mask_image)\n",
    "        mask, masked_image_latents = self.prepare_mask_latents(\n",
    "            mask,\n",
    "            masked_image,\n",
    "            batch_size * num_images_per_prompt,\n",
    "            height,\n",
    "            width,\n",
    "            prompt_embeds.dtype,\n",
    "            device,\n",
    "            generator,\n",
    "            do_classifier_free_guidance,\n",
    "        )\n",
    "\n",
    "        # 7. Prepare extra step kwargs. TODO: Logic should ideally just be moved out of the pipeline\n",
    "        extra_step_kwargs = self.prepare_extra_step_kwargs(generator, eta)\n",
    "\n",
    "        # 8. Denoising loop\n",
    "        num_warmup_steps = len(timesteps) - num_inference_steps * self.scheduler.order\n",
    "        with self.progress_bar(total=num_inference_steps) as progress_bar:\n",
    "            for i, t in enumerate(timesteps):\n",
    "                # expand the latents if we are doing classifier free guidance\n",
    "                latent_model_input = torch.cat([latents] * 2) if do_classifier_free_guidance else latents\n",
    "                latent_model_input = self.scheduler.scale_model_input(latent_model_input, t)                                \n",
    "\n",
    "                down_block_res_samples, mid_block_res_sample = self.controlnet(\n",
    "                    latent_model_input,\n",
    "                    t,\n",
    "                    encoder_hidden_states=prompt_embeds,\n",
    "                    controlnet_cond=control_image,\n",
    "                    return_dict=False,\n",
    "                )\n",
    "\n",
    "                down_block_res_samples = [\n",
    "                    down_block_res_sample * controlnet_conditioning_scale\n",
    "                    for down_block_res_sample in down_block_res_samples\n",
    "                ]\n",
    "                mid_block_res_sample *= controlnet_conditioning_scale\n",
    "\n",
    "                # predict the noise residual\n",
    "                latent_model_input = torch.cat([latent_model_input, mask, masked_image_latents], dim=1)\n",
    "                noise_pred = self.unet(\n",
    "                    latent_model_input,\n",
    "                    t,\n",
    "                    encoder_hidden_states=prompt_embeds,\n",
    "                    cross_attention_kwargs=cross_attention_kwargs,\n",
    "                    down_block_additional_residuals=down_block_res_samples,\n",
    "                    mid_block_additional_residual=mid_block_res_sample,\n",
    "                ).sample\n",
    "\n",
    "                # perform guidance\n",
    "                if do_classifier_free_guidance:\n",
    "                    noise_pred_uncond, noise_pred_text = noise_pred.chunk(2)\n",
    "                    noise_pred = noise_pred_uncond + guidance_scale * (noise_pred_text - noise_pred_uncond)\n",
    "\n",
    "                # compute the previous noisy sample x_t -> x_t-1\n",
    "                latents = self.scheduler.step(noise_pred, t, latents, **extra_step_kwargs).prev_sample\n",
    "\n",
    "                # call the callback, if provided\n",
    "                if i == len(timesteps) - 1 or ((i + 1) > num_warmup_steps and (i + 1) % self.scheduler.order == 0):\n",
    "                    progress_bar.update()\n",
    "                    if callback is not None and i % callback_steps == 0:\n",
    "                        callback(i, t, latents)\n",
    "\n",
    "        # If we do sequential model offloading, let's offload unet and controlnet\n",
    "        # manually for max memory savings\n",
    "        if hasattr(self, \"final_offload_hook\") and self.final_offload_hook is not None:\n",
    "            self.unet.to(\"cpu\")\n",
    "            self.controlnet.to(\"cpu\")\n",
    "            torch.cuda.empty_cache()\n",
    "\n",
    "        if output_type == \"latent\":\n",
    "            image = latents\n",
    "            has_nsfw_concept = None\n",
    "        elif output_type == \"pil\":\n",
    "            # 8. Post-processing\n",
    "            image = self.decode_latents(latents)\n",
    "\n",
    "            # 9. Run safety checker\n",
    "            image, has_nsfw_concept = self.run_safety_checker(image, device, prompt_embeds.dtype)\n",
    "\n",
    "            # 10. Convert to PIL\n",
    "            image = self.numpy_to_pil(image)\n",
    "        else:\n",
    "            # 8. Post-processing\n",
    "            image = self.decode_latents(latents)\n",
    "\n",
    "            # 9. Run safety checker\n",
    "            image, has_nsfw_concept = self.run_safety_checker(image, device, prompt_embeds.dtype)\n",
    "\n",
    "        # Offload last model to CPU\n",
    "        if hasattr(self, \"final_offload_hook\") and self.final_offload_hook is not None:\n",
    "            self.final_offload_hook.offload()\n",
    "\n",
    "        if not return_dict:\n",
    "            return (image, has_nsfw_concept)\n",
    "\n",
    "        return StableDiffusionPipelineOutput(images=image, nsfw_content_detected=has_nsfw_concept)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'load_image' from 'diffusers.utils' (/Users/kevinbuhler/opt/anaconda3/envs/blur/lib/python3.11/site-packages/diffusers/utils/__init__.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mdiffusers\u001b[39;00m \u001b[39mimport\u001b[39;00m StableDiffusionInpaintPipeline, ControlNetModel, UniPCMultistepScheduler\n\u001b[1;32m      2\u001b[0m \u001b[39m# from src.pipeline_stable_diffusion_controlnet_inpaint import *\u001b[39;00m\n\u001b[0;32m----> 3\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mdiffusers\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mutils\u001b[39;00m \u001b[39mimport\u001b[39;00m load_image\n\u001b[1;32m      5\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mcv2\u001b[39;00m\n\u001b[1;32m      6\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mPIL\u001b[39;00m \u001b[39mimport\u001b[39;00m Image\n",
      "\u001b[0;31mImportError\u001b[0m: cannot import name 'load_image' from 'diffusers.utils' (/Users/kevinbuhler/opt/anaconda3/envs/blur/lib/python3.11/site-packages/diffusers/utils/__init__.py)"
     ]
    }
   ],
   "source": [
    "from diffusers import StableDiffusionInpaintPipeline, ControlNetModel, UniPCMultistepScheduler\n",
    "# from src.pipeline_stable_diffusion_controlnet_inpaint import *\n",
    "from diffusers.utils import load_image\n",
    "\n",
    "import cv2\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import torch\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "blur",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
